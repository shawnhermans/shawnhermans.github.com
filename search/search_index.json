{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Shawn Hermans \u00b6 Hi! My name is Shawn Hermans. I plan to update this site later, but that might not happen.","title":"Home"},{"location":"#shawn-hermans","text":"Hi! My name is Shawn Hermans. I plan to update this site later, but that might not happen.","title":"Shawn Hermans"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/","text":"Originally posted on Quora . Note: In the time between now and when I originally wrote this answer, I have had considerable experience developing using Django Rest Framework and think it is one of the best Django-based API frameworks available. A while back I implemented a REST-based API in Django. I looked into a lot of options before I ended up writing my own. I first started by using Django Piston. I also looked at Django REST framework, but it seemed to be a little light on documentation. I was able to get a basic REST API up and running in a short period of time. My primary issue with Piston was that I had a difficult time trying to get it to support custom serialization. My API needed to support geospatial encoding in both GeoJSON and GeoRSS/GeoAtom. I tried to get the custom serialization to work in Piston, but did not have much luck. Additionally, I was using Django Haystack with Elasticsearch within my project.I wanted the API to be able to use Elasticsearch for advanced search and filtering. Right now I am in the process of re-evaluting these frameworks. Based upon my current research, I plan to explore TastyPie first. It seems to have better support for custom serialization methods. Also, it supports geospatial using GeoJSON. Out of the three frameworks it appears to have the best documentation. They have a section on using Haystack as well. Lastly, out of all of those options TastyPie has the best name. I guess the forth option not listed here is rolling your own without a framework. I went with this option with mixed results. The overall experience was useful in that it taught me a lot about designing and implementing REST APIs. Summary \u00b6 Django Piston \u00b6 Advantages - Easy to get setup and started. Works well if you use the default configuration. Disadvantages - Had a hard time implementing custom serialization methods. Django REST Framework \u00b6 Advantages - I didn't actually use this Disadvantages - Seems to have the least documentation out of the the three. TastyPie \u00b6 Advantages - Best name out of the three options. Seems to have the most features for implementing customized serialization and tying into non-ORM sources of data. Disadvantages - The reason I avoided TastyPie the first time around is it seemed too complex for my needs. TastyPie might be overkill for simple use cases. Rolling your own \u00b6 Advantages - Path of least resistance if you need to get something up and running for a proof of concept and don't care about having a lot of features. Disadvantages - End up doing a lot of rework. Time is better spent learning or improving existing framework.","title":"Django Rest Framework, TastyPie, and Piston Compared"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/#summary","text":"","title":"Summary"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/#django-piston","text":"Advantages - Easy to get setup and started. Works well if you use the default configuration. Disadvantages - Had a hard time implementing custom serialization methods.","title":"Django Piston"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/#django-rest-framework","text":"Advantages - I didn't actually use this Disadvantages - Seems to have the least documentation out of the the three.","title":"Django REST Framework"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/#tastypie","text":"Advantages - Best name out of the three options. Seems to have the most features for implementing customized serialization and tying into non-ORM sources of data. Disadvantages - The reason I avoided TastyPie the first time around is it seemed too complex for my needs. TastyPie might be overkill for simple use cases.","title":"TastyPie"},{"location":"posts/2012-07-20-drf-v-piston-v-tastypie/#rolling-your-own","text":"Advantages - Path of least resistance if you need to get something up and running for a proof of concept and don't care about having a lot of features. Disadvantages - End up doing a lot of rework. Time is better spent learning or improving existing framework.","title":"Rolling your own"},{"location":"posts/2012-07-20-nosql-hate/","text":"Originally posted as an answer to If some people in the NoSQL community hate SQL, why do they hate SQL? There are two possible ways to look at this question. First, why do some people in the NoSQL community hate SQL (the language). Second, why do some people in the NoSQL community hate relational databases. For the most part, I think people are really not talking about SQL the language, but rather relational databases vs non-relational databases. Personally, I am an advocate for NoSQL solutions, but I do not hate relational databases. NoSQL vs SQL seems to be one of those polarizing issues within the computer world. To me it is a lot like static vs dynamic typing. I think a lot of the hatred stems from a few issues. NoSQLers think that SQLers hate them. There is no shortage of people claiming that NoSQL is a fad and that people who use NoSQL are too stupid to understand relational databases. In this case, hate spawns more hate. SQL is typically associated with companies like Oracle and Microsoft whereas NoSQL is associated with companies like Google and Facebook. Google and Facebook are much cooler than Oracle and Microsoft. Relational databases are a lot more complex than NoSQL solutions. A lot of web developers want to be web developers and not DBAs. Now there are some people who bring up good, well thought out reasons for choosing NoSQL over SQL. However, I would say most of those people do not hate SQL, but just think NoSQL is better suited for some jobs.","title":"SQL Hate in the NoSQL Community"},{"location":"posts/2012-07-24-backend-python/","text":"Originally posted as an answer to How can I become a web developer using Python on the back end? I can't give you a recommendation on a holistic class, but I can give you some recommendations on individual pieces/parts. First, I think that Django is a great framework for learning Python web programming. It has great documentation and support. Working through the tutorial will give you a good understanding of what Django can do. Also, reading through the Django book will give you a good understanding of the additional features of Django. My personal opinion is to start with a project in mind and start coding. PyCharm is a good editor and it is only $29 for students. It even gives you some good tools for CSS and HTML. Otherwise, PyDev is a pretty good Eclipse based editor. The next steps are dependent on how interactive you want to make your web applications. If you don't care about doing a lot of fancy AJAX stuff, then you can probably do a lot of what you want with CSS, HTML and Django. I worked through my first few applications without needing any JavaScript. You can build some great applications with Django without ever needing to touch JavaScript. If you do want more dynamic applications, I would suggest working through some jQuery tutorials ((http://docs.jquery.com/Main_Page). Combining Django and jQuery is really simple. For the most part, the Django portions of my application really don't affect the jQuery portions. From my own experience, this worked out fairly well. I learned web development on my own outside of any class room setting. I started out working through the Django tutorials and books and did fine. I am sorry if I didn't give you a good, one-stop-shop for learning Python web development. In my experience, frontend development and backend development are orthogonal efforts. I generally do a few mockups using standalone HTML, CSS, and JavaScript. Once, I get the look and feel right, I convert that HTML over to a Django template.","title":"Python Backend Development"},{"location":"posts/2012-07-24-location-based-nosql/","text":"Originally posted as an answer to Which NoSQL Database would be a good backend for a location-based app? As mentioned before MongoDB and CouchDB both have geospatial support. Apache Solr and Elasticsearch are search engines which can double as a NoSQL data store. One option is to use a traditional database (or NoSQL datastore) with Elasticsearch or Solr as an external search index. Elasticsearch does a slightly better job of doing high volumes of real-time updates. Elasticsearch supports what are called rivers. Basically, rivers are a way to update Elasticsearch in real-time. They have rivers for RSS, Twitter, Mongo, and Couch.","title":"NoSQL Database for location-based app?"},{"location":"posts/2012-08-01-xslt-for-webaps/","text":"Originally posted as an answer to Why XSLT is not popular for developing web applications? The first problem with XSLT is that it is a programming language that where you have to write using XML rather than traditional programming paradigms. Most programmers are familiar with C-like languages (Java, C#, C). XML is horrible to write. I don't know many programmers who want to write code using XML. The second problem is that XSLT only applies to a very narrow set of problems. XSLT can transform XML documents into HTML, XML, or text. But you can do the same thing in Java, C, JavaScript, Python, C# or any other major programming language. What does XSLT give me that I can't do in any other major language? If I really like the functional features of XSLT, why not use a fully developed functional programming language? Lastly, XSLT is a relatively new language. XSLT came out as a W3C recommendation in 1999. By comparison, Java and JavaScript came out in 1995, C in 1972, C++ in 1983, and Python in 1994. It takes a long time for languages to reach widespread adoption.","title":"Using XSLT for Web Applications"},{"location":"posts/2012-08-15-storing-passwords-in-databases/","text":"Originally posted as an answer to Encryption: What is the best way to store username and password in a database? The previous answers to this question were very good, but I want to make sure I stress two things. When it comes to storing passwords using hashes, always use SALT (pepper is optional :). The reason is because if you only use a hash function, someone could use a rainbow table to break short or common passwords. The second thing I would recommend is never, ever, ever invent your own way of doing this. Use something like bcrypt or some other library that has been fully tested and vetted. Why? Cryptographic systems are hard to create. I would suggest reading http://www.schneier.com/blog/archives/2011/04/schneiers_law.html to give some perspective.","title":"Storing Passwords in Databases"},{"location":"posts/2012-08-30-postgis-v-spatialite/","text":"Originally posted as an answer to How does PostGIS compare to Spatialite? PostGIS offers many more features. GeoDjango's documentation has a comparison of the spatial features it supports from each database. Please note that this is not a direct comparison of the databases. Rather it reflects what GeoDjango supports. However, it will still give you a rough idea of the differences between the two. The choice between PostGIS and Spatialite comes down to your particular use case. Spatialite is probably the best choice when you need a standalone, embedded database. For example, maybe you are building a mobile or desktop application. Also, Spatialite may be used during the development of a web-based application. I have used it during the development of GeoDjango applications. PostGIS is the better choice if you need high read/write rates, multiple database users, clustering, network accessibility and any other standard database feature. Regarding Spatialite's licensing, the library is licensed under the LGPL and the GUI tools are licensed under the GPL (the same license as MySQL). This means in most cases, you do not need to worry about the viral nature of GPL.","title":"PostGIS vs Spatialite"},{"location":"posts/2013-01-17-coordinate-conversion-with-cython/","text":"One of my projects required conversion of Earth Centered Earth Fixed coordinates to geodetic latitude, longitude and height (and vice versa). I couldn\u2019t find any native Python libraries, so I decided to create a simple library called pycoordconvert . I decided this was a good opportunity to learn more about Cython . I created two versions of the code. One with Python and one with Cython extensions. This is the Python version of the code. It uses the built-in Python math libraries and does not use any explicitly declared types. from math import atan2 , sqrt , cos , sin , degrees , radians earth_radius = 6378.1 * 1000 a = 6378137 e2 = 6.6943799901E-3 def normal ( lat_radians ): return a / ( sqrt ( 1 - e2 * pow ( sin ( lat_radians ), 2 ))) def geodetic_to_ecef ( lon , lat , h ): lon = radians ( lon ) lat = radians ( lat ) x = ( normal ( lat ) + h ) * cos ( lon ) * cos ( lat ) y = ( normal ( lat ) + h ) * cos ( lon ) * sin ( lat ) z = ( normal ( lat ) * ( 1 - e2 ) + h ) * sin ( lat ) return x , y , z This is the equivalent code in Cython. It uses the C math libraries and static typing. cdef extern from \"math.h\" : double cos ( double theta ) double sin ( double theta ) double acos ( double theta ) double sqrt ( double theta ) double atan2 ( double y , double x ) double pow ( double x , double y ) cdef double earth_radius = 6378.1 * 1000 cdef double a = 6378137 cdef double e2 = 6.6943799901E-3 cdef double pi = 3.141592653589793 cdef double degrees_per_radian = 57.2957795 cdef double radians_per_degree = 0.0174532925 cdef double degrees ( double theta ): return degrees_per_radian * theta cdef double radians ( double theta ): return radians_per_degree * theta cdef double normal ( lat_radians ): return a / ( sqrt ( 1 - e2 * pow ( sin ( lat_radians ), 2 ))) cdef object geodetic_to_ecef ( lon , lat , h ): cdef double lon_rads , lat_rads , x , y , z lon_rads = radians ( lon ) lat_rads = radians ( lat ) x = ( normal ( lat_rads ) + h ) * cos ( lon_rads ) * cos ( lat_rads ) y = ( normal ( lat_rads ) + h ) * cos ( lon_rads ) * sin ( lat_rads ) z = ( normal ( lat_rads ) * ( 1 - e2 ) + h ) * sin ( lat_rads ) return ( x , y , z ) The reverse transforms can be found in the pycoordconvert repository. The following setup file enables building of the Cython module. from distutils.core import setup from distutils.extension import Extension from Cython.Distutils import build_ext setup ( cmdclass = { 'build_ext' : build_ext }, ext_modules = [ Extension ( \"cython_coordconvert\" , [ \"cython_coordconvert.pyx\" ])] ) Build the extensions using the command python setup . py build_ext --inplace . A quick comparison of the two approaches shows a significant speedup for a small amount of work. The ECEF to Geodetic ran in 4.41 usec/pass in Cython and 39.81 in CPython for 9x speedup. The Geodetic to ECEF ran in 1.01 usec/pass in Cython and 4.84 for CPython for a 4.8x speedup. Overall my first experience with Cython was favorable. It was a lot easier than writing native C code and provided a huge performance improvement.","title":"Coordinate Conversion in Cython"},{"location":"posts/2013-01-19-pigs-in-space/","text":"Introduction \u00b6 Alas, the subject of this post is about using Apache Pig to process satellite orbit data. If you are an orbital analyst and want to know how to easily process data on a Hadoop cluster, this article is for you. If you are just interested in Pig, this post should give you a good idea of how to use Pig and write Jython-based Pig user defined functions. Setting Up Pig \u00b6 Pig can run in either local mode or Hadoop mode. I am not going to go into great detail on how to setup Pig using Hadoop mode. Hadoop mode requires a fully configured Hadoop cluster with MapReduce version 1. If you are using Cloudera or some other Hadoop distribution, I recommend using whatever release of Pig that comes bundled with their distribution. Setting up Pig in standalone mode is much more straightforward. To setup Pig in standalone mode, download the latest Pig release . At the time of this writing, the latest Pig version is 10.1. Next, set the JAVA_HOME environment variable. On an Ubuntu based system, add the following into $ HOME / . profile . JAVA_HOME export JAVA_HOME =/ usr / jvm / default - java Decompress the Pig distribution. Common locations include / opt / pig or / usr / local / pig . This directory is the PIG_HOME directory. Starting Pig in standalone mode is as simple as executing the command $ PIG_HOME / bin / pig - x local . This will drop you into the Pig interactive shell Grunt. You are now ready to start processing some data in Pig! Getting and Preparing the Data \u00b6 There are a few sources of orbital data available. Space-Track.org hosts the full public catalog and contains roughly 14,000 objects. Access to this data requires registration and may not be readily accessible to all users. As an alternative, Celestrak hosts publicly available, supplemental TLEs. I will use these TLEs with the rest of the examples. Because Celestrak uses the same format as Space-Track.org, these examples should work equally as well against the full satellite catalog. Download the GPS ops TLEs. Despite what the name two-line element set would have you believe, most element sets come with three lines. The first line is the name of the object and the other two lines are the traditional TLE orbital elements. A typical file looks like this. GPS BIIA - 10 ( PRN 32 ) 1 20959 C 90103 A 13019 . 82648148 . 00000000 00000 - 0 ... 2 20959 54 . 4316 229 . 8752 0117400 335 . 3880 9 . 5445 ... GPS BIIA - 14 ( PRN 26 ) 1 22014 C 92039 A 13019 . 82648148 . 00000000 00000 - 0 ... 2 22014 56 . 1808 290 . 0954 0208678 69 . 4189 64 . 1323 ... . . . The problem with this format is that Pig, and most other data processing tools, assume one record per line. I could develop a custom Pig file loader, but that will have to wait for a later time. Instead, I created a simple Python script to convert the traditional format into a Pig friendly tab-separated value format. f_out = open ( 'gps-ops.tsv' , 'wb' ) output_line = '' with open ( 'gps-ops.txt' ) as f : for line_number , line in enumerate ( f ): if line_number % 3 != 2 : output_line += line . strip () + ' \\t ' else : output_line += line f_out . write ( output_line ) output_line = '' f_out . close () After processing, the file now looks like this. GPS BIIA - 10 ( PRN 32 ) 1 20959 U ... 8413 2 20959 ... 13 GPS BIIA - 14 ( PRN 26 ) 1 22014 U ... 6335 2 22014 ... 17 . . . The data is now in a Pig friendly format. Loading the Data \u00b6 Our first step is loading the data into Pig. I suggest creating a Pig workspace folder. Copy all data files and scripts into this directory. Then, while in this directory, execute pig - x local . grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); This statement creates a relation called gps . It loads data from the local file gps - ops . tsv . PigStorage () looks for the data on the local file system if in local mode. It loads from the HDFS filesystem if run in MapReduce mode. Loading the data as ( name : chararray , line1 : chararray , line2 : chararray ) tells Pig there are three fields and these fields are all strings. If I left that last part off, Pig would still load the data, but would interpret the columns as untyped byte arrays. This is part of Pig\u2019s philosophy of \u201cpigs eat anything\u201d. If you have type information, Pig will gladly use it. If you don\u2019t, Pig doesn\u2019t care. At this point, let\u2019s see what Pig has actually done. The DESCRIBE command will tell us information on the relation. This is not too useful right now, but comes in handy when dealing with derived relationships. grunt > DESCRIBE gps ; gps : { name : chararray , line1 : chararray , line2 : chararray } We can take a look at the data using the DUMP command. grunt > DUMP gps ; ( GPS BIIA - 10 ( PRN 32 ), 1 ... - 3 0 8413 , 2 ... 2 . 00550870162199 ) ( GPS BIIA - 14 ( PRN 26 ), 1 ... - 3 0 6335 , 2 ... 2 . 00565888143921 ) . . . Creating a Python User Defined Function \u00b6 While the data is currently in Pig, it requires further parsing before we can do anything useful with it. The data is encoded in the positions within the string. For example, columns 15-17 in line 1 represent the international designator. Lines 45-52 are the second time derivative of mean motion divided by six with the decimal point assumed. We can\u2019t do much else with the data until we parse it. While it may be possible to parse this using built-in Pig capabilities, it is a whole lot easier parsing this is something like Python. Luckily, Pig supports creating user defined functions (UDFs) using Java, JavaScript, Python and Ruby. Pig UDFs can use most external Python libraries provided they do not have any C dependencies. This eliminates popular data processing libraries such as Lxml and NumPy. Additionally, external libraries cannot rely on Python features found in releases after 2.5. This is because Pig UDFs use Jython to interpret Python UDFs. The upside of using Jython is that Pig UDFs can call Java classes as if they were native Python libraries. I will demonstrate this feature later on. I created a simple UDF called parseTle that takes in the name, line1, and line2 data and parses it into a map. A map is one of Pig\u2019s complex data types and is equivalent to a Python dictionary. import tleparser @outputSchema ( \"params:map[]\" ) def parseTle ( name , line1 , line2 ): params = tleparser . parse_tle ( name , line1 , line2 ) return params The UDF is pretty simple to understand. It imports an external library called tleparser to do most of the heavy lifting. The @outputSchema ( \"params:map[]\" ) annotation tells Pig this function outputs a map named params. The easiest way to use this UDF is to place the UDF script and all dependencies in the working folder. Create the files tleUDFs . py and tleparser . py in this folder and restart the Grunt console. grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); grunt > REGISTER 'tleUDFs.py' USING jython AS myfuncs ; grunt > parsed = FOREACH gps GENERATE myfuncs . parseTle (*); This Pig command goes through each of the relations in gps and applies the parseTle UDF. The parsed data is now a map of key/value pairs. The following is an example parsed TLE data. ( [ bstar # , arg_of_perigee # 333 . 0924 , mean_motion # 2 . 00559335 , element_number # 72 , epoch_year # 2013 , inclination # 54 . 9673 , mean_anomaly # 26 . 8787 , rev_at_epoch # 210 , mean_motion_ddot # 0 . 0 , eccentricity # 5 . 354 E - 4 , two_digit_year # 13 , international_designator # 12053 A , classification # U , epoch_day # 17 . 78040066 , satellite_number # 38833 , name # GPS BIIF - 3 ( PRN 24 ), mean_motion_dot #- 1 . 8 E - 6 , ra_of_asc_node # 344 . 5315 ] ) Pig uses the # character to separate key/value pairs. Having the data parsed, means we can use the power of Pig to filter, process, and otherwise manipulate it. The following is an example of using the FILTER command to find all orbits with inclination between 53 and 56. filtered = FILTER parsed BY ( params # 'inclination' > 53 ) AND ( params # 'inclination' < 56 ); At this point, we can store the results of this processing to the filesystem using the STORE command. STORE filtered into '/tmp/filtered' using PigStorage (); After running this command, there should be a folder named / tmp / filtered with a file called part-m-00000. For larger data sets with more reducers, this folder will contain multiple files using the same naming convention. This file is a tab-separated representation of the filtered data. This data can be loaded back into Pig using the PigStorage option. Making a Pig Script \u00b6 Up until now, all of the examples used the Grunt interactive console. This is very useful for conducting data exploration, but not very useful for doing repeatable tasks. Luckily, creating a Pig script is as easy as saving those commands into a text file and executing it using the Pig commandline. To create really useful scripts we want to vary certain parameters at runtime. This is accomplished using Pig parameter substitution. Save the previous commands into a file called filter . pig , but change the last line to include the following. REGISTER 'tleUDFs.py' USING jython AS tleUDFs ; gps = LOAD '$input' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); parsed = FOREACH gps GENERATE tleUDFs . parseTle (*); filtered = FILTER parsed BY ( params # 'inclination' > $ lowerInclination ) AND ( params # 'inclination' < $ upperInclination ); STORE filtered into '$output' using PigStorage (); This allows the user to pass in lower and upper inclination bounds. It also stores the results into a user defined output directory. We pass those parameters using the commandline - param option. pig - x local - param input = 'gps-ops.tsv' - param lowerInclination = 53 \\ - param upperInclination = 56 - param output = '/tmp/filtered' filter . pig Using Java in Python UDFs \u00b6 By themselves, orbital elements are not terribly useful. The most common use case for TLEs is to use the TLEs to propagate out predicted positions and velocities for objects in space. This requires access to a propagator that implements all the relevant mathematics and physics. Ideally, we want to create UDF that propagates out the TLE. Usually, when I want to propagate a TLE in Python, I use PyEphem . Unfortunately, I can\u2019t use PyEphem to write Pig UDFs. Pig uses Jython to interpret Python UDFs. Using Jython means you can\u2019t use any Python code requiring C extensions. It also means you are limited to using standard library functions found in Python 2.5 as Jython does not support the latest versions of Python. On the other hand, Jython allows for seamless integration with existing Java code. This means you can reuse existing Java libraries or rewrite computationally intensive code in Java. This UDF uses the JSatTrak Java library to propagate out the TLE. To use this UDF download the JSatTrak binary distribution . Next copy the JARs from this distribution somewhere onto the Pig classpath. The easiest way to do this is copying the JARs into the $ PIG_HOME / lib directory. from jsattrak.objects import SatelliteTleSGP4 @outputSchema ( \"\"\" propagated:bag{ positions:tuple( time:double, x:double, y:double, z:double ) } \"\"\" ) def propagateTleECEF ( name , line1 , line2 , start_time , end_time , number_of_points ): try : satellite = SatelliteTleSGP4 ( name , line1 , line2 ) except : return None ecef_positions = [] end_time = float ( end_time ) start_time = float ( start_time ) number_of_points = float ( number_of_points ) increment = ( end_time - start_time ) / number_of_points current_time = start_time while current_time <= end_time : try : positions = [ current_time ] current_positions = satellite . calculateJ2KPositionFromUT ( current_time ) current_positions = list ( current_positions ) positions . extend ( current_positions ) ecef_positions . append ( tuple ( positions )) except : pass current_time += increment return ecef_positions This UDF takes a start time, end time and the number of desired points and outputs a bag of tuples. A bag is similar to a Python list with the exception that bags in Pig are always unordered. Tuples in Pig are like tuples in Python. They are immutable and ordered. After adding the JARs to the classpath and adding the propagateTleECEF function to tleUDFs . py , we are ready to propagate some orbits. grunt > REGISTER 'tleUDFs.py' USING jython AS myfuncs ; grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); grunt > propagated = FOREACH gps GENERATE myfuncs . parseTle ( name , line1 , line2 ), myfuncs . propagateTleECEF ( name , line1 , line2 , 2454992 . 0 , 2454993 . 0 , 100 ); grunt > flattened = FOREACH propagated GENERATE params # 'satellite_number' , FLATTEN ( propagated ); The third command propagates out points for each satellite while the fourth command flattens the results so that each position for each satellite is on its own line. The FLATTEN operator will take a bag or tuple and flatten it. This operator works differently based on whether it is flattening a bag or a tuple. Tuples are flattened in place in will not generate additional relations. Bags are flattened into multiple additional relations. The DESCRIBE command shows the new structure of each relation. grunt > DESCRIBE propagated ; propagated : { params : map [], propagated : { positions : ( time : double , x : double , y : double , z : double ) }} grunt > DESCRIBE flattened ; flattened : { bytearray , propagated :: time : double , propagated :: x : double , propagated :: y : double , propagated :: z : double } Notice that the flattened relation has an unnamed, untyped field for its first item. This is the satellite number, but Pig does not know its type or name so it treats it as an untyped byte array. We can fix this by using the AS operator to specify its name and type. Looking at the data using the dump flattened command shows the positions. ( 38833 , 2454992 . 9599999785 , 2 . 278136816721697 E7 , 7970303 . 195970464 , - 1 . 1066153998664627 E7 ) ( 38833 , 2454992 . 9699999783 , 2 . 2929498370345607 E7 , 1 . 0245812732430315 E7 , - 8617450 . 742994161 ) ( 38833 , 2454992 . 979999978 , 2 . 2713614118860725 E7 , 1 . 2358665040019082 E7 , - 6031915 . 392826946 ) ( 38833 , 2454992 . 989999978 , 2 . 213715624812226 E7 , 1 . 4275325605036272 E7 , - 3350605 . 7983842064 ) ( 38833 , 2454992 . 9999999776 , 2 . 1209296863515433 E7 , 1 . 5965381866069315 E7 , - 616098 . 4598421039 ) At this point we can store off the propagated data so we can use it for future processing in Pig or in an external program. Using STORE flattened into 'propagated' using PigStorage ( ',' ) stores the data in the folder \u2018propagated\u2019 using comma-separated value files. Pig has other storage options as well such as HBase or JSON. Pig also allows development of custom storage drivers. We can load this data back into Pig using LOAD . grunt > propagated = LOAD 'propagated' using PigStorage ( ',' ) AS ( satelliteNumber : int , time : double , x : double , y : double , z : double ); Using the \u2018AS\u2019 operator allows us to define the schema for the data. grunt > DESCRIBE propagated ; propagated : { satelliteNumber : int , time : double , x : double , y : double , z : double } Conclusion \u00b6 Overall, Pig made it easy to process orbital data without needing to resort to cumbersome custom MapReduce code. It handles all the boring details of creating and scheduling MapReduce jobs. Pig is a valuable tool and will be a critical part of my data processing toolbox. This post only scratches the surface of the true power of Pig. As an example, I started down the path of developing a simple conjunction assessment algorithm (conjunctions are when satellites in space get too close to one another and go boom) using a Python UDF. It ran fine against small data sets, but ran out of heap memory on larger data sets. I suspect I should be able to fine tune the processing to run efficiently against the whole data set.","title":"Pigs in Space"},{"location":"posts/2013-01-19-pigs-in-space/#introduction","text":"Alas, the subject of this post is about using Apache Pig to process satellite orbit data. If you are an orbital analyst and want to know how to easily process data on a Hadoop cluster, this article is for you. If you are just interested in Pig, this post should give you a good idea of how to use Pig and write Jython-based Pig user defined functions.","title":"Introduction"},{"location":"posts/2013-01-19-pigs-in-space/#setting-up-pig","text":"Pig can run in either local mode or Hadoop mode. I am not going to go into great detail on how to setup Pig using Hadoop mode. Hadoop mode requires a fully configured Hadoop cluster with MapReduce version 1. If you are using Cloudera or some other Hadoop distribution, I recommend using whatever release of Pig that comes bundled with their distribution. Setting up Pig in standalone mode is much more straightforward. To setup Pig in standalone mode, download the latest Pig release . At the time of this writing, the latest Pig version is 10.1. Next, set the JAVA_HOME environment variable. On an Ubuntu based system, add the following into $ HOME / . profile . JAVA_HOME export JAVA_HOME =/ usr / jvm / default - java Decompress the Pig distribution. Common locations include / opt / pig or / usr / local / pig . This directory is the PIG_HOME directory. Starting Pig in standalone mode is as simple as executing the command $ PIG_HOME / bin / pig - x local . This will drop you into the Pig interactive shell Grunt. You are now ready to start processing some data in Pig!","title":"Setting Up Pig"},{"location":"posts/2013-01-19-pigs-in-space/#getting-and-preparing-the-data","text":"There are a few sources of orbital data available. Space-Track.org hosts the full public catalog and contains roughly 14,000 objects. Access to this data requires registration and may not be readily accessible to all users. As an alternative, Celestrak hosts publicly available, supplemental TLEs. I will use these TLEs with the rest of the examples. Because Celestrak uses the same format as Space-Track.org, these examples should work equally as well against the full satellite catalog. Download the GPS ops TLEs. Despite what the name two-line element set would have you believe, most element sets come with three lines. The first line is the name of the object and the other two lines are the traditional TLE orbital elements. A typical file looks like this. GPS BIIA - 10 ( PRN 32 ) 1 20959 C 90103 A 13019 . 82648148 . 00000000 00000 - 0 ... 2 20959 54 . 4316 229 . 8752 0117400 335 . 3880 9 . 5445 ... GPS BIIA - 14 ( PRN 26 ) 1 22014 C 92039 A 13019 . 82648148 . 00000000 00000 - 0 ... 2 22014 56 . 1808 290 . 0954 0208678 69 . 4189 64 . 1323 ... . . . The problem with this format is that Pig, and most other data processing tools, assume one record per line. I could develop a custom Pig file loader, but that will have to wait for a later time. Instead, I created a simple Python script to convert the traditional format into a Pig friendly tab-separated value format. f_out = open ( 'gps-ops.tsv' , 'wb' ) output_line = '' with open ( 'gps-ops.txt' ) as f : for line_number , line in enumerate ( f ): if line_number % 3 != 2 : output_line += line . strip () + ' \\t ' else : output_line += line f_out . write ( output_line ) output_line = '' f_out . close () After processing, the file now looks like this. GPS BIIA - 10 ( PRN 32 ) 1 20959 U ... 8413 2 20959 ... 13 GPS BIIA - 14 ( PRN 26 ) 1 22014 U ... 6335 2 22014 ... 17 . . . The data is now in a Pig friendly format.","title":"Getting and Preparing the Data"},{"location":"posts/2013-01-19-pigs-in-space/#loading-the-data","text":"Our first step is loading the data into Pig. I suggest creating a Pig workspace folder. Copy all data files and scripts into this directory. Then, while in this directory, execute pig - x local . grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); This statement creates a relation called gps . It loads data from the local file gps - ops . tsv . PigStorage () looks for the data on the local file system if in local mode. It loads from the HDFS filesystem if run in MapReduce mode. Loading the data as ( name : chararray , line1 : chararray , line2 : chararray ) tells Pig there are three fields and these fields are all strings. If I left that last part off, Pig would still load the data, but would interpret the columns as untyped byte arrays. This is part of Pig\u2019s philosophy of \u201cpigs eat anything\u201d. If you have type information, Pig will gladly use it. If you don\u2019t, Pig doesn\u2019t care. At this point, let\u2019s see what Pig has actually done. The DESCRIBE command will tell us information on the relation. This is not too useful right now, but comes in handy when dealing with derived relationships. grunt > DESCRIBE gps ; gps : { name : chararray , line1 : chararray , line2 : chararray } We can take a look at the data using the DUMP command. grunt > DUMP gps ; ( GPS BIIA - 10 ( PRN 32 ), 1 ... - 3 0 8413 , 2 ... 2 . 00550870162199 ) ( GPS BIIA - 14 ( PRN 26 ), 1 ... - 3 0 6335 , 2 ... 2 . 00565888143921 ) . . .","title":"Loading the Data"},{"location":"posts/2013-01-19-pigs-in-space/#creating-a-python-user-defined-function","text":"While the data is currently in Pig, it requires further parsing before we can do anything useful with it. The data is encoded in the positions within the string. For example, columns 15-17 in line 1 represent the international designator. Lines 45-52 are the second time derivative of mean motion divided by six with the decimal point assumed. We can\u2019t do much else with the data until we parse it. While it may be possible to parse this using built-in Pig capabilities, it is a whole lot easier parsing this is something like Python. Luckily, Pig supports creating user defined functions (UDFs) using Java, JavaScript, Python and Ruby. Pig UDFs can use most external Python libraries provided they do not have any C dependencies. This eliminates popular data processing libraries such as Lxml and NumPy. Additionally, external libraries cannot rely on Python features found in releases after 2.5. This is because Pig UDFs use Jython to interpret Python UDFs. The upside of using Jython is that Pig UDFs can call Java classes as if they were native Python libraries. I will demonstrate this feature later on. I created a simple UDF called parseTle that takes in the name, line1, and line2 data and parses it into a map. A map is one of Pig\u2019s complex data types and is equivalent to a Python dictionary. import tleparser @outputSchema ( \"params:map[]\" ) def parseTle ( name , line1 , line2 ): params = tleparser . parse_tle ( name , line1 , line2 ) return params The UDF is pretty simple to understand. It imports an external library called tleparser to do most of the heavy lifting. The @outputSchema ( \"params:map[]\" ) annotation tells Pig this function outputs a map named params. The easiest way to use this UDF is to place the UDF script and all dependencies in the working folder. Create the files tleUDFs . py and tleparser . py in this folder and restart the Grunt console. grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); grunt > REGISTER 'tleUDFs.py' USING jython AS myfuncs ; grunt > parsed = FOREACH gps GENERATE myfuncs . parseTle (*); This Pig command goes through each of the relations in gps and applies the parseTle UDF. The parsed data is now a map of key/value pairs. The following is an example parsed TLE data. ( [ bstar # , arg_of_perigee # 333 . 0924 , mean_motion # 2 . 00559335 , element_number # 72 , epoch_year # 2013 , inclination # 54 . 9673 , mean_anomaly # 26 . 8787 , rev_at_epoch # 210 , mean_motion_ddot # 0 . 0 , eccentricity # 5 . 354 E - 4 , two_digit_year # 13 , international_designator # 12053 A , classification # U , epoch_day # 17 . 78040066 , satellite_number # 38833 , name # GPS BIIF - 3 ( PRN 24 ), mean_motion_dot #- 1 . 8 E - 6 , ra_of_asc_node # 344 . 5315 ] ) Pig uses the # character to separate key/value pairs. Having the data parsed, means we can use the power of Pig to filter, process, and otherwise manipulate it. The following is an example of using the FILTER command to find all orbits with inclination between 53 and 56. filtered = FILTER parsed BY ( params # 'inclination' > 53 ) AND ( params # 'inclination' < 56 ); At this point, we can store the results of this processing to the filesystem using the STORE command. STORE filtered into '/tmp/filtered' using PigStorage (); After running this command, there should be a folder named / tmp / filtered with a file called part-m-00000. For larger data sets with more reducers, this folder will contain multiple files using the same naming convention. This file is a tab-separated representation of the filtered data. This data can be loaded back into Pig using the PigStorage option.","title":"Creating a Python User Defined Function"},{"location":"posts/2013-01-19-pigs-in-space/#making-a-pig-script","text":"Up until now, all of the examples used the Grunt interactive console. This is very useful for conducting data exploration, but not very useful for doing repeatable tasks. Luckily, creating a Pig script is as easy as saving those commands into a text file and executing it using the Pig commandline. To create really useful scripts we want to vary certain parameters at runtime. This is accomplished using Pig parameter substitution. Save the previous commands into a file called filter . pig , but change the last line to include the following. REGISTER 'tleUDFs.py' USING jython AS tleUDFs ; gps = LOAD '$input' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); parsed = FOREACH gps GENERATE tleUDFs . parseTle (*); filtered = FILTER parsed BY ( params # 'inclination' > $ lowerInclination ) AND ( params # 'inclination' < $ upperInclination ); STORE filtered into '$output' using PigStorage (); This allows the user to pass in lower and upper inclination bounds. It also stores the results into a user defined output directory. We pass those parameters using the commandline - param option. pig - x local - param input = 'gps-ops.tsv' - param lowerInclination = 53 \\ - param upperInclination = 56 - param output = '/tmp/filtered' filter . pig","title":"Making a Pig Script"},{"location":"posts/2013-01-19-pigs-in-space/#using-java-in-python-udfs","text":"By themselves, orbital elements are not terribly useful. The most common use case for TLEs is to use the TLEs to propagate out predicted positions and velocities for objects in space. This requires access to a propagator that implements all the relevant mathematics and physics. Ideally, we want to create UDF that propagates out the TLE. Usually, when I want to propagate a TLE in Python, I use PyEphem . Unfortunately, I can\u2019t use PyEphem to write Pig UDFs. Pig uses Jython to interpret Python UDFs. Using Jython means you can\u2019t use any Python code requiring C extensions. It also means you are limited to using standard library functions found in Python 2.5 as Jython does not support the latest versions of Python. On the other hand, Jython allows for seamless integration with existing Java code. This means you can reuse existing Java libraries or rewrite computationally intensive code in Java. This UDF uses the JSatTrak Java library to propagate out the TLE. To use this UDF download the JSatTrak binary distribution . Next copy the JARs from this distribution somewhere onto the Pig classpath. The easiest way to do this is copying the JARs into the $ PIG_HOME / lib directory. from jsattrak.objects import SatelliteTleSGP4 @outputSchema ( \"\"\" propagated:bag{ positions:tuple( time:double, x:double, y:double, z:double ) } \"\"\" ) def propagateTleECEF ( name , line1 , line2 , start_time , end_time , number_of_points ): try : satellite = SatelliteTleSGP4 ( name , line1 , line2 ) except : return None ecef_positions = [] end_time = float ( end_time ) start_time = float ( start_time ) number_of_points = float ( number_of_points ) increment = ( end_time - start_time ) / number_of_points current_time = start_time while current_time <= end_time : try : positions = [ current_time ] current_positions = satellite . calculateJ2KPositionFromUT ( current_time ) current_positions = list ( current_positions ) positions . extend ( current_positions ) ecef_positions . append ( tuple ( positions )) except : pass current_time += increment return ecef_positions This UDF takes a start time, end time and the number of desired points and outputs a bag of tuples. A bag is similar to a Python list with the exception that bags in Pig are always unordered. Tuples in Pig are like tuples in Python. They are immutable and ordered. After adding the JARs to the classpath and adding the propagateTleECEF function to tleUDFs . py , we are ready to propagate some orbits. grunt > REGISTER 'tleUDFs.py' USING jython AS myfuncs ; grunt > gps = LOAD 'gps-ops.tsv' USING PigStorage () AS ( name : chararray , line1 : chararray , line2 : chararray ); grunt > propagated = FOREACH gps GENERATE myfuncs . parseTle ( name , line1 , line2 ), myfuncs . propagateTleECEF ( name , line1 , line2 , 2454992 . 0 , 2454993 . 0 , 100 ); grunt > flattened = FOREACH propagated GENERATE params # 'satellite_number' , FLATTEN ( propagated ); The third command propagates out points for each satellite while the fourth command flattens the results so that each position for each satellite is on its own line. The FLATTEN operator will take a bag or tuple and flatten it. This operator works differently based on whether it is flattening a bag or a tuple. Tuples are flattened in place in will not generate additional relations. Bags are flattened into multiple additional relations. The DESCRIBE command shows the new structure of each relation. grunt > DESCRIBE propagated ; propagated : { params : map [], propagated : { positions : ( time : double , x : double , y : double , z : double ) }} grunt > DESCRIBE flattened ; flattened : { bytearray , propagated :: time : double , propagated :: x : double , propagated :: y : double , propagated :: z : double } Notice that the flattened relation has an unnamed, untyped field for its first item. This is the satellite number, but Pig does not know its type or name so it treats it as an untyped byte array. We can fix this by using the AS operator to specify its name and type. Looking at the data using the dump flattened command shows the positions. ( 38833 , 2454992 . 9599999785 , 2 . 278136816721697 E7 , 7970303 . 195970464 , - 1 . 1066153998664627 E7 ) ( 38833 , 2454992 . 9699999783 , 2 . 2929498370345607 E7 , 1 . 0245812732430315 E7 , - 8617450 . 742994161 ) ( 38833 , 2454992 . 979999978 , 2 . 2713614118860725 E7 , 1 . 2358665040019082 E7 , - 6031915 . 392826946 ) ( 38833 , 2454992 . 989999978 , 2 . 213715624812226 E7 , 1 . 4275325605036272 E7 , - 3350605 . 7983842064 ) ( 38833 , 2454992 . 9999999776 , 2 . 1209296863515433 E7 , 1 . 5965381866069315 E7 , - 616098 . 4598421039 ) At this point we can store off the propagated data so we can use it for future processing in Pig or in an external program. Using STORE flattened into 'propagated' using PigStorage ( ',' ) stores the data in the folder \u2018propagated\u2019 using comma-separated value files. Pig has other storage options as well such as HBase or JSON. Pig also allows development of custom storage drivers. We can load this data back into Pig using LOAD . grunt > propagated = LOAD 'propagated' using PigStorage ( ',' ) AS ( satelliteNumber : int , time : double , x : double , y : double , z : double ); Using the \u2018AS\u2019 operator allows us to define the schema for the data. grunt > DESCRIBE propagated ; propagated : { satelliteNumber : int , time : double , x : double , y : double , z : double }","title":"Using Java in Python UDFs"},{"location":"posts/2013-01-19-pigs-in-space/#conclusion","text":"Overall, Pig made it easy to process orbital data without needing to resort to cumbersome custom MapReduce code. It handles all the boring details of creating and scheduling MapReduce jobs. Pig is a valuable tool and will be a critical part of my data processing toolbox. This post only scratches the surface of the true power of Pig. As an example, I started down the path of developing a simple conjunction assessment algorithm (conjunctions are when satellites in space get too close to one another and go boom) using a Python UDF. It ran fine against small data sets, but ran out of heap memory on larger data sets. I suspect I should be able to fine tune the processing to run efficiently against the whole data set.","title":"Conclusion"},{"location":"posts/2013-02-27-nosql-design-considerations/","text":"Originally posted as an answer to What are some good resources for people considering using a NoSQL database for a web app? There are a lot of NoSQL options out there so things like design considerations will depend a lot on the type of NoSQL option and your use cases. Reasons why you might want to use NoSQL \u00b6 Your data does not have a well-defined schema. NoSQL options normally deal with schema-less data a lot better than traditional RDBMS systems. You have an application that requires high write rates. As an example, real-time analytics is a good example where a NoSQL solution is a good option. As an example, something like Redis is going to give you high read/write rates as compared to traditional SQL options. Your application requires an advanced full-text search capability. In these situations something like Apache Solr or Elasticsearch might be a good choice. SQL databases generally require more expertise to manage and scale. NoSQL options generally require less expertise. Therefore NoSQL options may be the right choice for a startup effort. Y* our data is highly linked and you want to do social network analysis. A graph database might be a good choice. Design Considerations \u00b6 Most NoSQL databases do not support JOINS very well. This means you will need to avoid many-to-many relationships. This means you may need to redesign your current schema. There are some good object relational mapping tools for NoSQL, but they are not as mature as SQL ORMs. I generally do web development in Python, so there is Django Nonrel, MongoEngine and I think there are a few others. I noticed Spring had a few tools available for NoSQL Java options. You may want to consider some hybrid approaches depending upon your needs. One of the other posters mentioned combining CouchDB with Redis. That is a pretty good design pattern. In general using an in-memory cache like Redis or Memcached with a persistent data store works well for a lot of applications. Or maybe you could use a NoSQL search engine like Elasticsearch with a traditional database. NoSQL options vary in their reliability. Some support transactions and some do not. Most support a model of eventually consistency, but check into the specifics of your NoSQL choice. If you are looking to replace a traditional SQL database with a NoSQL one, SimpleDB, MongoDB, CouchDB, Google App Engine\u2019s datastore, and HBase are decent options. HBase is probably the most reliable and well supported of them. SimpleDB and Google App Engine are both cloud options. MongoDB and CouchDB are both document-oriented datastores. Both are good at storing JSON or JSON-like data structures. This makes them good for designing REST APIs. If you are looking to augment a web app with NoSQL capabilities, Elasticsearch is a good option for adding full-text search. Redis is a great option for in-memory caching. Both Elasticsearch and Redis are very simple and have good performance.","title":"NoSQL Design Considerations"},{"location":"posts/2013-02-27-nosql-design-considerations/#reasons-why-you-might-want-to-use-nosql","text":"Your data does not have a well-defined schema. NoSQL options normally deal with schema-less data a lot better than traditional RDBMS systems. You have an application that requires high write rates. As an example, real-time analytics is a good example where a NoSQL solution is a good option. As an example, something like Redis is going to give you high read/write rates as compared to traditional SQL options. Your application requires an advanced full-text search capability. In these situations something like Apache Solr or Elasticsearch might be a good choice. SQL databases generally require more expertise to manage and scale. NoSQL options generally require less expertise. Therefore NoSQL options may be the right choice for a startup effort. Y* our data is highly linked and you want to do social network analysis. A graph database might be a good choice.","title":"Reasons why you might want to use NoSQL"},{"location":"posts/2013-02-27-nosql-design-considerations/#design-considerations","text":"Most NoSQL databases do not support JOINS very well. This means you will need to avoid many-to-many relationships. This means you may need to redesign your current schema. There are some good object relational mapping tools for NoSQL, but they are not as mature as SQL ORMs. I generally do web development in Python, so there is Django Nonrel, MongoEngine and I think there are a few others. I noticed Spring had a few tools available for NoSQL Java options. You may want to consider some hybrid approaches depending upon your needs. One of the other posters mentioned combining CouchDB with Redis. That is a pretty good design pattern. In general using an in-memory cache like Redis or Memcached with a persistent data store works well for a lot of applications. Or maybe you could use a NoSQL search engine like Elasticsearch with a traditional database. NoSQL options vary in their reliability. Some support transactions and some do not. Most support a model of eventually consistency, but check into the specifics of your NoSQL choice. If you are looking to replace a traditional SQL database with a NoSQL one, SimpleDB, MongoDB, CouchDB, Google App Engine\u2019s datastore, and HBase are decent options. HBase is probably the most reliable and well supported of them. SimpleDB and Google App Engine are both cloud options. MongoDB and CouchDB are both document-oriented datastores. Both are good at storing JSON or JSON-like data structures. This makes them good for designing REST APIs. If you are looking to augment a web app with NoSQL capabilities, Elasticsearch is a good option for adding full-text search. Redis is a great option for in-memory caching. Both Elasticsearch and Redis are very simple and have good performance.","title":"Design Considerations"},{"location":"posts/2014-07-18-big-data-introduction/","text":"Yesterday I gave an introductory talk on Big Data to the Heartland Big Data Meetup Group . There was a great turnout for the talk. I had a lot of fun talking with so many people with an interest in Big Data. I posted the slides on SlideShare if you want to take a look. Thinking Big with Big Data from Shawn Hermans","title":"Heartland Meetup Group: Intro to Big Data"},{"location":"posts/2014-07-28-big-data-reading-list/","text":"These are some of the books and blogs I have read in the last few years to get up to speed on Big Data, data science, and related concepts. Hadoop \u00b6 Hadoop the Definitive Guide - As the title indicates, this is the definitive guide for Hadoop. It provides a great introduction to the Hadoop ecosystem. While it covers things like Pig and Hive, it does not go into depth on any one topic. Hadoop Operations - This book is a must have for any Hadoop administrator. It documents a bunch of practical advice on administering Hadoop clusters not found in any other literature. Programming Pig - This is one of the few books on the Pig language. It provides a good introduction to Pig. Practical Data Mining and Machine Learning \u00b6 These books give a hands on introduction to working with data and machine learning algorithms. Data Analysis with Open Source Tools - This is a great book for getting a practical introduction to data analysis using Python and other open source tools. Mahout in Action - This is one of the only books out there on using the Mahout library. It is invaluable if you need to do any programming using the Mahout libraries. Data Mining: Practical Machine Learning Tools and Techniques - A very practical introduction to machine learning algorithms. Think Bayes - Practical introduction to Bayesian data analysis. Statistics and Algorithms Theory \u00b6 These books provide the theoretical background behind algorithms used in Big Data. These are not the best books for learning practical application. Instead, these books provide great references for when you need to understand the inner workings of different algorithms. Introduction to Algorithms - When you are really working with Big Data, it is essential to have a good understanding of algorithms and how they scale. At the very least, you should know Big O Notation and how it applies to algorithms and data structures. Elements of Statistical Learning - This book provides a great reference for machine learning algorithms including Random Forest, logistic regression, k-means, and ensemble learning. Bayesian Data Analysis - This book provides a solid introduction to the theory behind Bayesian methods. Data-Intensive Text Processing with MapReduce - This online book describes how to implement different algorithms using the MapReduce paradigm. This is very useful for understanding how to approach algorithm implementation in MapReduce. Non-Technical \u00b6 These books are non-technical, but provide a good background for ways of thinking about data. Signal and the Noise - This book is a great introduction to thinking about data in terms of Bayesian statistics. The author provides many real world examples of applying Bayesian statistics including sports and politics. Thinking, Fast and Slow - This book is not specifically about statistical thinking, but about how people make decisions. It covers many of the common cognitive bias that cause people to make incorrect conclusions. Avoiding bias is essential for anyone interested in doing data analysis. Antifragile - Nassim Taleb's book Antifragile and his previous book The Black Swan , both explore the limitations of statistics in prediction. The Grand Design - This is a book about modern physics, but more than that it is about using observed data to create descriptive models of our universe. This book introduced me to the concept of model-dependent realism . This approach claims that we can only understand the universe via intermediate models. Websites and Newsletters \u00b6 These are some of the websites and newsletters I frequent on a regular basis. DataTau - Datatau is a news aggregation site focused on data related topics. Data Science Weekly - Weekly newsletter concerning all things data science. Hacker Newsletter - A weekly newsletter highlighting some of the best articles on Hacker News for the week. Quora - A question/answer site with a very active data community. Good place to go to ask data engineering and data science questions. FiveThirtyEight - This site applies statistical analysis sports, news, politics and life. Farnam Street - Covers a wide variety of topics from a variety of disciplines. It isn't focused on data, but a great blog for those interested in learning about multiple topics. Articles/Tutorials \u00b6 Google Research - This is a collection of various Google Research papers. It covers artificial intelligence, machine learning, distributed systems, and much more. The Fourth Quadrant - An essay by Nassim Taleb on the fundamental limitations of using statistics in making decisions. Numbers Everybody Should Know - While this entire presentation is interesting, slide 13 is the one anyone working with data at scale should understand. It provides some latency statistics for read/write operations.","title":"My Data Reading List"},{"location":"posts/2014-07-28-big-data-reading-list/#hadoop","text":"Hadoop the Definitive Guide - As the title indicates, this is the definitive guide for Hadoop. It provides a great introduction to the Hadoop ecosystem. While it covers things like Pig and Hive, it does not go into depth on any one topic. Hadoop Operations - This book is a must have for any Hadoop administrator. It documents a bunch of practical advice on administering Hadoop clusters not found in any other literature. Programming Pig - This is one of the few books on the Pig language. It provides a good introduction to Pig.","title":"Hadoop"},{"location":"posts/2014-07-28-big-data-reading-list/#practical-data-mining-and-machine-learning","text":"These books give a hands on introduction to working with data and machine learning algorithms. Data Analysis with Open Source Tools - This is a great book for getting a practical introduction to data analysis using Python and other open source tools. Mahout in Action - This is one of the only books out there on using the Mahout library. It is invaluable if you need to do any programming using the Mahout libraries. Data Mining: Practical Machine Learning Tools and Techniques - A very practical introduction to machine learning algorithms. Think Bayes - Practical introduction to Bayesian data analysis.","title":"Practical Data Mining and Machine Learning"},{"location":"posts/2014-07-28-big-data-reading-list/#statistics-and-algorithms-theory","text":"These books provide the theoretical background behind algorithms used in Big Data. These are not the best books for learning practical application. Instead, these books provide great references for when you need to understand the inner workings of different algorithms. Introduction to Algorithms - When you are really working with Big Data, it is essential to have a good understanding of algorithms and how they scale. At the very least, you should know Big O Notation and how it applies to algorithms and data structures. Elements of Statistical Learning - This book provides a great reference for machine learning algorithms including Random Forest, logistic regression, k-means, and ensemble learning. Bayesian Data Analysis - This book provides a solid introduction to the theory behind Bayesian methods. Data-Intensive Text Processing with MapReduce - This online book describes how to implement different algorithms using the MapReduce paradigm. This is very useful for understanding how to approach algorithm implementation in MapReduce.","title":"Statistics and Algorithms Theory"},{"location":"posts/2014-07-28-big-data-reading-list/#non-technical","text":"These books are non-technical, but provide a good background for ways of thinking about data. Signal and the Noise - This book is a great introduction to thinking about data in terms of Bayesian statistics. The author provides many real world examples of applying Bayesian statistics including sports and politics. Thinking, Fast and Slow - This book is not specifically about statistical thinking, but about how people make decisions. It covers many of the common cognitive bias that cause people to make incorrect conclusions. Avoiding bias is essential for anyone interested in doing data analysis. Antifragile - Nassim Taleb's book Antifragile and his previous book The Black Swan , both explore the limitations of statistics in prediction. The Grand Design - This is a book about modern physics, but more than that it is about using observed data to create descriptive models of our universe. This book introduced me to the concept of model-dependent realism . This approach claims that we can only understand the universe via intermediate models.","title":"Non-Technical"},{"location":"posts/2014-07-28-big-data-reading-list/#websites-and-newsletters","text":"These are some of the websites and newsletters I frequent on a regular basis. DataTau - Datatau is a news aggregation site focused on data related topics. Data Science Weekly - Weekly newsletter concerning all things data science. Hacker Newsletter - A weekly newsletter highlighting some of the best articles on Hacker News for the week. Quora - A question/answer site with a very active data community. Good place to go to ask data engineering and data science questions. FiveThirtyEight - This site applies statistical analysis sports, news, politics and life. Farnam Street - Covers a wide variety of topics from a variety of disciplines. It isn't focused on data, but a great blog for those interested in learning about multiple topics.","title":"Websites and Newsletters"},{"location":"posts/2014-07-28-big-data-reading-list/#articlestutorials","text":"Google Research - This is a collection of various Google Research papers. It covers artificial intelligence, machine learning, distributed systems, and much more. The Fourth Quadrant - An essay by Nassim Taleb on the fundamental limitations of using statistics in making decisions. Numbers Everybody Should Know - While this entire presentation is interesting, slide 13 is the one anyone working with data at scale should understand. It provides some latency statistics for read/write operations.","title":"Articles/Tutorials"},{"location":"posts/2015-12-10-the-learning-organization/","text":"What does it mean to learn? How do we learn things Are machines capable of learning? Can organizations learn things? There are no simple answers to any of these questions. Breakthroughs in neuroscience are giving us better insight into how the human brain works, but we are nowhere close to understanding the complexity involved in the human brain. Advances in artificial intelligence techniques enable wonders such as Apple's Siri and IBM's Watson, but we are nowhere near the HAL 9000 predicted in 2001: A Space Odyssey. What we do know is our ability to learn is what makes humans unique among all other life on this planet. It is not our ability to learn new and complex tasks that make us unique. Other animals can learn to perform complex tasks. What makes us unique is our ability to transfer that knowledge and those skills to other people. Human society advances because we build upon the knowledge and experience of our ancestors. For humans, the value of learning does not come from an individual acquiring skills and knowledge. The value comes when individuals pass that knowledge and skills on to others. With this in mind, we can think of learning as something that happens at an individual level and at a group level. Not only do people learn, but companies learn, markets learn, governments learn, and families learn. Any organized group of people will naturally learn. However, this does not mean they learn quickly, efficiently, or even learn the right lessons. Organizations learn similarly to the way humans learn. Humans collect data with their senses and transmit that data to the brain via the nervous system. The brain takes the information from the various sources, integrates it, learns from it and responds accordingly. In the case of an organization, its senses include the data being collected by dozens, if not hundreds, of different information systems. It also includes the documented and undocumented knowledge of all its members. Its nervous system is the meetings, emails, and all other forms of formal and informal communication. The brain of the organization, arguably the most important part of the organization, is not always as well developed as it should be. In humans, the nervous system is responsible for transporting signals to the brain for processing. In this respect, many large organizations resemble a Tyrannosaurus Rex, rather than a human. Recent studies indicate that rather than being a fast, fearsome predator, the T. rex likely plodded along like an elephant. Like modern elephants, the T. rex could not move any faster, because its nerves could not transfer the data fast enough. A similar problem occurs organizations. By the time the information reaches someone who can do something about it, it is already too late. In his book, The Hard Thing About Hard Things, Ben Horowitz makes the observation that in an organization with a healthy culture, bad news travels fast and good news travels slow. These organizations have a good nervous system. For an organization to learn and adapt, it must have good senses, a fast nervous system, and a brain to make sense of it all. Many organizations have primitive brains that primarily acts on reflex and instinct. The information from all of the various senses is not integrated into a unified picture of the world. The nervous system does a suboptimal job of distributing information to all of the people and systems who need it.","title":"The Organization that Learns"},{"location":"posts/2016-02-29-extracting-text-data-python/","text":"Working with text data \u00b6 If you have conducted any type of data analysis, you probably have worked with typical numeric and text data types. Microsoft Excel supports various numeric and text types. If you work with databases, you have probably used decimals, floating point numbers, integers, and character types. While the standard data types are good enough for most applications, sometimes you have text data with some sort of structure. Social security numbers, mailing addresses, phone numbers, email addresses, and website URLs are some typical examples. In these cases, treating the data as unstructured text might not be good enough. If you are examining data with telephone numbers, you might want to compare records within the same area code. If the area code is not stored in its own field, you will need to extract from the full phone number. Maybe you want to look at records by zipcode, but all you have is a mailing address. At Bellevue University, there are various examples of useful information being embedded in text data. Course names follow a naming convention that contains the program code, course number, section number, and term information. In the Skills to Performance model, information is embedded in the naming convention for the skills and rubrics. Extracting information from this type of data is exceedingly useful to analytics efforts. In addition to extracting information from text fields, it is often necessary to validate that a text field conforms to a particular naming convention. If data does not conform to the naming convention, it might prevent analysis of that data. At the very least, data that does not conform to the naming convention causes extra work for everyone involved and should be avoided when possible. This document provides a short introduction on how to use the Python programming language to work with semi-structured text. It uses examples from Bellevue University's Skills to Performance naming convention. This tutorial demonstrates how to use Python to determine whether a skill name adheres to the naming convention. Furthermore, it shows how to extract information embedded in the skill name. Skill naming conventions \u00b6 The Skills to Performance model evaluates student performance based upon mastery of skills. Courses within the same program may share common skill definitions. Conversely, a skill definition might only apply to a single course. This is often the case with general education courses that do not belong to a particular program. This tutorial will look at two different skill identifier formats. One format is used for general education courses and the other is used for skills attached to a program. General education skill name \u00b6 General education skills uses the following naming conventions: < course_abbr > _ < course_num > _ < gen_ed_cat > . < skill > . < subskill > < course_abbr > _ < course_num > _ < gen_ed_cat > . < skill > HI_150_HC . 01 . 02 is an example of skill definition for a general education course. This example uses the first format. HI_150_HC . 01 is a case without a subskill. It uses the second form. course_abbr is the abbreviated name for the course. This abbreviation should be no shorter than two characters and no longer than five characters. The characters should always be upper case letters. course_num is the three digit number for the course. gen_ed_cat is the general education category for the course. It is two upper case letters. skill is the two digit number identifying the skill. It must contain a leading zero if the number is less than 10. subskill is the two digit number identifying the subskill. It must include a leading zero if the number is less than 10. Subskill is optional if the parent skill does not define any subskills. If no subskills are defined, the second form should be used. Program skill name \u00b6 Skills attached to a program have the following naming conventions: < program_abbr > _ < course_num > _ < perf_obj > . < skill > . < subskill > < program_abbr > _ < course_num > _ < perf_obj > . < skill > PPSY_300_01 . 01 . 02 is an example of skill definition for a course belonging to a program. This example uses the first format. PPSY_300_01 . 01 is a case without a subskill. It uses the second form. program_abbr is the abbreviated name for the program. This abbreviation should be no shorter than two characters and no longer than five characters. The characters should always be upper case letters. course_num is the three digit number for the course. perf_obj is the performance objective number for the skill. It is always two digits and must contain a leading zero if the number is less than 10. skill is the two digit number identifying the skill. It must contain a leading zero if the number is less than 10. subskill is the two digit number identifying the subskill. It must include a leading zero if the number is less than 10. Subskill is optional if the parent skill does not define any subskills. If no subskills are defined, the second form should be used. Validating and parsing skill names \u00b6 The code presented in this tutorial has been simplified to help those who are not familiar with Python. It avoids advanced Python features and is heavily commented. It only uses features found the Python 3 standard library. If you want to run the examples given in this document, you will need to install Python 3 on your system. As of this writing, Python 3.5 is the latest stable version of Python 3. If you are new to Python, I recommend using the Anaconda distribution of Python produced by Continuum Analytics. It provides graphical installers for both Windows and Mac OS X. It also includes Jupyter notebook ; a locally run web application for creating documents powered by Python. In fact, I created this document with Jupyter notebook. Regular expressions \u00b6 The code in this tutorial makes heavy use of regular expressions. Regular expressions are usefully for matching patterns in text. Consider a social security number as an example. A social security number is three digits followed by a hyphen followed by two digits followed by another hyphen followed by four digits. Explaining patterns like this in English is rather long and clumsy. Defining this pattern as a regular expression is much more compact. The regular expression (regex for short) for a social security number is: ^ ([ 0 - 9 ] { 3 } ) - ([ 0 - 9 ] { 2 } ) - ([ 0 - 9 ] { 4 } ) $ Don't feel dejected if this looks like an incomprehensible jumble of numbers and symbols. As conveyed in this XCKD comic , even experienced developers struggle with regular expressions. While regular expressions can be frustrating at times, they are a powerful tool when working with partially structured data. Python code \u00b6 The following section describes the skill parsing code. If you want to try it out for yourself, this skill_parser.py gist provides the Python used to define the skill parsing code. If you want the code, unit tests, and examples, it is all bundled in this Jupyter notebook . Select the option to Download ZIP , unzip the contents, and open the contents using Jupyter notebook. The end of this tutorial provides brief instructions on how to download and install Jupyter notebook. A few examples \u00b6 The following shows how to use the code to validate and parse skill names. General education skill without subskill \u00b6 {% highlight python %} gen_ed_skill = 'HI_150_HC.01' parse_skill(gen_ed_skill) {% endhighlight %} Which returns the result: { 'course' : 'HI' , 'course_number' : '150' , 'gen_ed_category' : 'HC' , 'skill' : '01' , 'subskill' : None } General education skill with subskill \u00b6 {% highlight python %} gen_ed_with_subskill = 'HI_150_HC.01.02' parse_skill(gen_ed_with_subskill) {% endhighlight %} Which returns the result: { 'course' : 'HI' , 'course_number' : '150' , 'gen_ed_category' : 'HC' , 'skill' : '01' , 'subskill' : '02' } Program skill without subskill \u00b6 {% highlight python %} program_skill = 'PPSY_300_01.01' parse_skill(program_skill) {% endhighlight %} Which returns the result: { 'course_number' : '300' , 'performance_objective' : '01' , 'program' : 'PPSY' , 'skill' : '01' , 'subskill' : None } Program skill with subskill \u00b6 {% highlight python %} program_with_subskill = 'PPSY_300_01.01.02' parse_skill(program_with_subskill) {% endhighlight %} Which returns the result: { 'course_number' : '300' , 'performance_objective' : '01' , 'program' : 'PPSY' , 'skill' : '01' , 'subskill' : '02' } Handling invalid input \u00b6 When we try to parse an skill that does not meet the format, it will raise a ValidationError. This lets the developer know something went wrong. {% highlight python %} try: # Python tries to execute the code in this block. # If the code runs without error, print out parsed info. # If the code raises a ValidationError, go to exception block invalid_skill = 'PPSY_300_01.1' print ( parse_skill ( invalid_skill )) except ValidationError as err: # Block catches ValidationError. Prints out error message print(err) {% endhighlight %} Which returns this result: 'PPSY_300_01.1' does not match known format Unit testing \u00b6 This section gives an overview of how to use unit testing in Python. Unit testing is a way of testing individual units of code. When a developer submits code changes, a testing service automatically runs these tests. The developer is not allowed to add the modifications to the project unless all the tests pass. The biggest advantage of unit testing (and automated testing in general) is it catches errors early in the development process. A simple test \u00b6 The following is a single unit test. It tests the parsing of a general education skill that does not contain any subskill. When this test runs, it evaluates the assertion statements. These statements are simple true/false statements. If all the statements evaluate to true, the tests pass. If an assertion fails (i.e. evaluates as false), the test fails. The following unit test should run without any errors. {% highlight python %} def test_parse_gen_ed_skill(): result = parse_skill('HI_150_HC.01') assert result [ 'course' ] == 'HI' assert result [ 'course_number' ] == '150' assert result [ 'gen_ed_category' ] == 'HC' assert result [ 'skill' ] == '01' assert result [ 'subskill' ] == None try: test_parse_gen_ed_skill() print('Test passed!') except AssertionError as err: print('Test failed :(') print(err) {% endhighlight %} Running this code should print: Test passed ! Failing the test \u00b6 The next example shows what happens when a test fails. It is similar to the test above but has a failing assertion. Specifically, the skill string \"PPSY_300_01.01\" should return the skill number of \"01\". In this particular example, we are telling it to match \"1\" and not \"01\". Since these are not the same values, the test should fail. {% highlight python %} def test_parse_program_skill(): result = parse_skill('PPSY_300_01.01') assert result [ 'program' ] == 'PPSY' assert result [ 'course_number' ] == '300' assert result [ 'performance_objective' ] == '01' # Define a message skill_error_msg = 'Received skill value ' skill_error_msg += '\"{0}\". ' . format ( result [ 'skill' ]) skill_error_msg += 'Expected \"1\" as value.' assert result [ 'skill' ] == '1' , skill_error_msg try: test_parse_program_skill() print('Test passed!') except AssertionError as err: print('Test failed :(') print(err) {% endhighlight %} Running this code should result in a test failure. Test failed :( Received skill value \"01\" . Expected \"1\" as value . Next steps \u00b6 If you are interested in learning Python, there are plenty of resources available. Automate the boring stuff is a gentle introduction to Python that uses practical, real-world examples. Google's Python class is a great resource if you already have some programming background. Data Science from Scratch and Python for Data Analysis are good books for those with a background in analysis. If you just want to download Python and try it out, I recommend starting with the Anaconda distribution from Continuum Analytics. They provide graphical installers for Windows and Mac OS X . Choose the Python 3.5 installer if you are not sure which version to pick. They also provide installation instructions . Once you have it installed, open up Jupyter notebook and create a Python 3 notebook. You can use this notebook to experiment with Python code.","title":"Extracting Data from Text with Python"},{"location":"posts/2016-02-29-extracting-text-data-python/#working-with-text-data","text":"If you have conducted any type of data analysis, you probably have worked with typical numeric and text data types. Microsoft Excel supports various numeric and text types. If you work with databases, you have probably used decimals, floating point numbers, integers, and character types. While the standard data types are good enough for most applications, sometimes you have text data with some sort of structure. Social security numbers, mailing addresses, phone numbers, email addresses, and website URLs are some typical examples. In these cases, treating the data as unstructured text might not be good enough. If you are examining data with telephone numbers, you might want to compare records within the same area code. If the area code is not stored in its own field, you will need to extract from the full phone number. Maybe you want to look at records by zipcode, but all you have is a mailing address. At Bellevue University, there are various examples of useful information being embedded in text data. Course names follow a naming convention that contains the program code, course number, section number, and term information. In the Skills to Performance model, information is embedded in the naming convention for the skills and rubrics. Extracting information from this type of data is exceedingly useful to analytics efforts. In addition to extracting information from text fields, it is often necessary to validate that a text field conforms to a particular naming convention. If data does not conform to the naming convention, it might prevent analysis of that data. At the very least, data that does not conform to the naming convention causes extra work for everyone involved and should be avoided when possible. This document provides a short introduction on how to use the Python programming language to work with semi-structured text. It uses examples from Bellevue University's Skills to Performance naming convention. This tutorial demonstrates how to use Python to determine whether a skill name adheres to the naming convention. Furthermore, it shows how to extract information embedded in the skill name.","title":"Working with text data"},{"location":"posts/2016-02-29-extracting-text-data-python/#skill-naming-conventions","text":"The Skills to Performance model evaluates student performance based upon mastery of skills. Courses within the same program may share common skill definitions. Conversely, a skill definition might only apply to a single course. This is often the case with general education courses that do not belong to a particular program. This tutorial will look at two different skill identifier formats. One format is used for general education courses and the other is used for skills attached to a program.","title":"Skill naming conventions"},{"location":"posts/2016-02-29-extracting-text-data-python/#general-education-skill-name","text":"General education skills uses the following naming conventions: < course_abbr > _ < course_num > _ < gen_ed_cat > . < skill > . < subskill > < course_abbr > _ < course_num > _ < gen_ed_cat > . < skill > HI_150_HC . 01 . 02 is an example of skill definition for a general education course. This example uses the first format. HI_150_HC . 01 is a case without a subskill. It uses the second form. course_abbr is the abbreviated name for the course. This abbreviation should be no shorter than two characters and no longer than five characters. The characters should always be upper case letters. course_num is the three digit number for the course. gen_ed_cat is the general education category for the course. It is two upper case letters. skill is the two digit number identifying the skill. It must contain a leading zero if the number is less than 10. subskill is the two digit number identifying the subskill. It must include a leading zero if the number is less than 10. Subskill is optional if the parent skill does not define any subskills. If no subskills are defined, the second form should be used.","title":"General education skill name"},{"location":"posts/2016-02-29-extracting-text-data-python/#program-skill-name","text":"Skills attached to a program have the following naming conventions: < program_abbr > _ < course_num > _ < perf_obj > . < skill > . < subskill > < program_abbr > _ < course_num > _ < perf_obj > . < skill > PPSY_300_01 . 01 . 02 is an example of skill definition for a course belonging to a program. This example uses the first format. PPSY_300_01 . 01 is a case without a subskill. It uses the second form. program_abbr is the abbreviated name for the program. This abbreviation should be no shorter than two characters and no longer than five characters. The characters should always be upper case letters. course_num is the three digit number for the course. perf_obj is the performance objective number for the skill. It is always two digits and must contain a leading zero if the number is less than 10. skill is the two digit number identifying the skill. It must contain a leading zero if the number is less than 10. subskill is the two digit number identifying the subskill. It must include a leading zero if the number is less than 10. Subskill is optional if the parent skill does not define any subskills. If no subskills are defined, the second form should be used.","title":"Program skill name"},{"location":"posts/2016-02-29-extracting-text-data-python/#validating-and-parsing-skill-names","text":"The code presented in this tutorial has been simplified to help those who are not familiar with Python. It avoids advanced Python features and is heavily commented. It only uses features found the Python 3 standard library. If you want to run the examples given in this document, you will need to install Python 3 on your system. As of this writing, Python 3.5 is the latest stable version of Python 3. If you are new to Python, I recommend using the Anaconda distribution of Python produced by Continuum Analytics. It provides graphical installers for both Windows and Mac OS X. It also includes Jupyter notebook ; a locally run web application for creating documents powered by Python. In fact, I created this document with Jupyter notebook.","title":"Validating and parsing skill names"},{"location":"posts/2016-02-29-extracting-text-data-python/#regular-expressions","text":"The code in this tutorial makes heavy use of regular expressions. Regular expressions are usefully for matching patterns in text. Consider a social security number as an example. A social security number is three digits followed by a hyphen followed by two digits followed by another hyphen followed by four digits. Explaining patterns like this in English is rather long and clumsy. Defining this pattern as a regular expression is much more compact. The regular expression (regex for short) for a social security number is: ^ ([ 0 - 9 ] { 3 } ) - ([ 0 - 9 ] { 2 } ) - ([ 0 - 9 ] { 4 } ) $ Don't feel dejected if this looks like an incomprehensible jumble of numbers and symbols. As conveyed in this XCKD comic , even experienced developers struggle with regular expressions. While regular expressions can be frustrating at times, they are a powerful tool when working with partially structured data.","title":"Regular expressions"},{"location":"posts/2016-02-29-extracting-text-data-python/#python-code","text":"The following section describes the skill parsing code. If you want to try it out for yourself, this skill_parser.py gist provides the Python used to define the skill parsing code. If you want the code, unit tests, and examples, it is all bundled in this Jupyter notebook . Select the option to Download ZIP , unzip the contents, and open the contents using Jupyter notebook. The end of this tutorial provides brief instructions on how to download and install Jupyter notebook.","title":"Python code"},{"location":"posts/2016-02-29-extracting-text-data-python/#a-few-examples","text":"The following shows how to use the code to validate and parse skill names.","title":"A few examples"},{"location":"posts/2016-02-29-extracting-text-data-python/#general-education-skill-without-subskill","text":"{% highlight python %} gen_ed_skill = 'HI_150_HC.01' parse_skill(gen_ed_skill) {% endhighlight %} Which returns the result: { 'course' : 'HI' , 'course_number' : '150' , 'gen_ed_category' : 'HC' , 'skill' : '01' , 'subskill' : None }","title":"General education skill without subskill"},{"location":"posts/2016-02-29-extracting-text-data-python/#general-education-skill-with-subskill","text":"{% highlight python %} gen_ed_with_subskill = 'HI_150_HC.01.02' parse_skill(gen_ed_with_subskill) {% endhighlight %} Which returns the result: { 'course' : 'HI' , 'course_number' : '150' , 'gen_ed_category' : 'HC' , 'skill' : '01' , 'subskill' : '02' }","title":"General education skill with subskill"},{"location":"posts/2016-02-29-extracting-text-data-python/#program-skill-without-subskill","text":"{% highlight python %} program_skill = 'PPSY_300_01.01' parse_skill(program_skill) {% endhighlight %} Which returns the result: { 'course_number' : '300' , 'performance_objective' : '01' , 'program' : 'PPSY' , 'skill' : '01' , 'subskill' : None }","title":"Program skill without subskill"},{"location":"posts/2016-02-29-extracting-text-data-python/#program-skill-with-subskill","text":"{% highlight python %} program_with_subskill = 'PPSY_300_01.01.02' parse_skill(program_with_subskill) {% endhighlight %} Which returns the result: { 'course_number' : '300' , 'performance_objective' : '01' , 'program' : 'PPSY' , 'skill' : '01' , 'subskill' : '02' }","title":"Program skill with subskill"},{"location":"posts/2016-02-29-extracting-text-data-python/#handling-invalid-input","text":"When we try to parse an skill that does not meet the format, it will raise a ValidationError. This lets the developer know something went wrong. {% highlight python %} try: # Python tries to execute the code in this block. # If the code runs without error, print out parsed info. # If the code raises a ValidationError, go to exception block invalid_skill = 'PPSY_300_01.1' print ( parse_skill ( invalid_skill )) except ValidationError as err: # Block catches ValidationError. Prints out error message print(err) {% endhighlight %} Which returns this result: 'PPSY_300_01.1' does not match known format","title":"Handling invalid input"},{"location":"posts/2016-02-29-extracting-text-data-python/#unit-testing","text":"This section gives an overview of how to use unit testing in Python. Unit testing is a way of testing individual units of code. When a developer submits code changes, a testing service automatically runs these tests. The developer is not allowed to add the modifications to the project unless all the tests pass. The biggest advantage of unit testing (and automated testing in general) is it catches errors early in the development process.","title":"Unit testing"},{"location":"posts/2016-02-29-extracting-text-data-python/#a-simple-test","text":"The following is a single unit test. It tests the parsing of a general education skill that does not contain any subskill. When this test runs, it evaluates the assertion statements. These statements are simple true/false statements. If all the statements evaluate to true, the tests pass. If an assertion fails (i.e. evaluates as false), the test fails. The following unit test should run without any errors. {% highlight python %} def test_parse_gen_ed_skill(): result = parse_skill('HI_150_HC.01') assert result [ 'course' ] == 'HI' assert result [ 'course_number' ] == '150' assert result [ 'gen_ed_category' ] == 'HC' assert result [ 'skill' ] == '01' assert result [ 'subskill' ] == None try: test_parse_gen_ed_skill() print('Test passed!') except AssertionError as err: print('Test failed :(') print(err) {% endhighlight %} Running this code should print: Test passed !","title":"A simple test"},{"location":"posts/2016-02-29-extracting-text-data-python/#failing-the-test","text":"The next example shows what happens when a test fails. It is similar to the test above but has a failing assertion. Specifically, the skill string \"PPSY_300_01.01\" should return the skill number of \"01\". In this particular example, we are telling it to match \"1\" and not \"01\". Since these are not the same values, the test should fail. {% highlight python %} def test_parse_program_skill(): result = parse_skill('PPSY_300_01.01') assert result [ 'program' ] == 'PPSY' assert result [ 'course_number' ] == '300' assert result [ 'performance_objective' ] == '01' # Define a message skill_error_msg = 'Received skill value ' skill_error_msg += '\"{0}\". ' . format ( result [ 'skill' ]) skill_error_msg += 'Expected \"1\" as value.' assert result [ 'skill' ] == '1' , skill_error_msg try: test_parse_program_skill() print('Test passed!') except AssertionError as err: print('Test failed :(') print(err) {% endhighlight %} Running this code should result in a test failure. Test failed :( Received skill value \"01\" . Expected \"1\" as value .","title":"Failing the test"},{"location":"posts/2016-02-29-extracting-text-data-python/#next-steps","text":"If you are interested in learning Python, there are plenty of resources available. Automate the boring stuff is a gentle introduction to Python that uses practical, real-world examples. Google's Python class is a great resource if you already have some programming background. Data Science from Scratch and Python for Data Analysis are good books for those with a background in analysis. If you just want to download Python and try it out, I recommend starting with the Anaconda distribution from Continuum Analytics. They provide graphical installers for Windows and Mac OS X . Choose the Python 3.5 installer if you are not sure which version to pick. They also provide installation instructions . Once you have it installed, open up Jupyter notebook and create a Python 3 notebook. You can use this notebook to experiment with Python code.","title":"Next steps"},{"location":"posts/2016-07-18-bi-vs-data-science/","text":"Originally posted as an answer to How business intelligence and Data science is related? It is tempting to dismiss data science as a mere rebranding of data analysis, business intelligence, and statistics. In many cases, people use the term data science as a sexy synonym for statistics. But once you cut away all the hype, there are many differences between business intelligence and data science. First, I need to point that data science is relatively new, and we are still trying to figure out what exactly is and isn't data science. As such, there is no precise definition of data science that everyone agrees upon. This does not mean it is not a real thing, but rather it is an emerging discipline. In my experience, here are a few key differences between data science and business intelligence. Choice of tools - A data scientist often works with programming languages such as R or Python. A business analyst will probably be more comfortable with graphical tools like Tableau, SAS, or Excel. Type of data - Business intelligence typically uses structured data found in traditional relational databases, data warehouses, or spreadsheets. Data science uses non-traditional sources of data. This might include HTML scraped from websites, data obtained from web APIs, metadata extracted from files, and many other sources. Roles and responsibilities - In a typical BI environment, ETL is the responsibility of the information technology department, while business analysis is the responsibility of the business units. Data science combines those into a single role. A data scientist handles every phase of the data transformation pipeline. Data analysis and extraction methods - A data scientist is more likely to use more involved data analysis and extraction methods. Not just business - Data science is not limited to business problems. The tools and techniques are applied outside of the domain of business. That being said, a business analyst might have much more knowledge on a particular business area. However, the line between business intelligence and data science is often blurry. Both use data to inform decision-making but use different tools and techniques.","title":"Business Intelligence vs Data Science"},{"location":"posts/2017-02-23-migration-gcs-to-aws/","text":"Originally posted as an answer to How can I migrate data from Google cloud storage into AWS S3 buckets? Hadoop Distcp is a good way of moving large amounts of data between different file systems. Here are the steps I used to transfer data between Google Cloud Storage and S3 using distcp. I created a 3-node Hadoop cluster using Google Dataproc. If it is configured correctly, it should have access to your GCS files without having to add any additional configuration. Once the cluster finishes initializing, SSH into the master node and run the following command. hadoop distcp \\ http : // gs : //< bucket - name >/< folder >/ \\ http : // s3a : //< aws_access_key > : < aws_secret_key >@< s3 - bucket >/< folder > This should copy the data from GCS to S3.","title":"Migrating GCS to AWS S3"},{"location":"posts/2017-03-29-what-is-a-spark-dag/","text":"Originally posted as an answer to What is DAG in Spark, and how does it work? As Rajagopal ParthaSarathi pointed out, a DAG is a directed acyclic graph. They are commonly used in computer systems for task execution. In this context, a graph is a collection of nodes that are connected by edges. In the case of Hadoop and Spark, the nodes represent executable tasks, and the edges are task dependencies. Think of the DAG like a flow chart that tells the system which tasks to execute and in what order. The following is a simple example of an undirected graph of tasks. This graph is undirected because there it does not capture which node is the start node and which is the end node. In other words, this graph does not tell me if the reduce task should be feeding the map tasks or vice versa. The next graph shows a directed graph of tasks. A directed graph gives an unambiguous direction for each edge. This means that we know that the map tasks feed into the reduce task, rather than the other way around. This property is essential for executing complex workflows since we need to know which tasks should be executed in which order. Lastly, the graph is acyclic , because it does not contain any cycles. A cycle happens when it is possible to loop back to a previous node. Cycles are useful for tasks involving recursion but not as good for large-scale distributed systems. The following are two examples of graphs with cycles.","title":"What is DAG in Spark?"},{"location":"posts/2017-03-31-data-science-challenges/","text":"Originally posted as an answer to What are the most challenging problems you encounter in your work as a data scientist? Politics, process, and organization are some of the most challenging problems I have encountered as a data scientist. This is especially true for well-established organizations that are new to data science. Established organizations are not always the best at innovating and changing (see The Innovator\u2019s Dilemma for more information). This can hinder the implementation of a successful data science effort. Here are a few of the most challenging problems I have encountered. Centralized Information Technology : In my humble opinion, data science is valuable because it combines technology, statistics, mathematics, and business knowledge into a single role. Combining all of these responsibilities into a single role/organization is often difficult for established organizations that centralize all IT functions into a single organization. The IT organization will often push back on data science requirements because they are outside the scope of their normal requirements and workflow. It is not normal for them to have a business user creating databases, running large-scale ETL jobs, or standing up servers. Data ownership/governance : Data science usually involves gathering and combining data from multiple sources. This is difficult to accomplish when data is scattered across the organization, and there are no clear rules governing who can access what data. This means data owners ordinarily make arbitrary decisions on who can and cannot access their data. They might be hesitant to share this data with someone who they perceive as an outsider. Vendor-driven decision making : I have seen many vendors who promote \"data science in a box\" solutions. These vendors will give polished presentations that make it seem like their product can do everything and anything. Problems arise when the company gets all of their information from the vendors and does not get feedback from the end-users of the solutions. Too much focus on technology : Big companies have a tendency to adopt the latest and greatest trend without really understanding what it is all about. Service-Oriented Architecture (SOA) is an example of a trend that is great in principle but was frequently poorly implemented. Adoption of SOA often failed, because organizations focused on technology (web services, SOAP, REST, ...) and not on the architecture and process needed for SOA to be successful. The same holds true for data science. Data science is not about Hadoop, Spark, Tableau, or any other technology. These tools are a means to an end. A successful data science implementation regularly require changes to processes, procedures, policy, and organization. Boil the ocean approach : If you are not familiar with this phrase, \"boiling the ocean\" is when a task or a problem tries to solve too many things at once. I have also heard people use the phrases \"trying to solve world hunger\" and \"trying to run before learning how to walk.\" If an organization is new to data science, I recommend a small, focused piloting effort. The Lean Startup has some good advice on how to implement something like this in an existing organization. Lack of buy-in : All of the problems I have described above will worse if you do not have buy-in from the company's leadership. They need to understand why data science is valuable, what they are trying to achieve, and what needs to be done to achieve it. I am sorry if you were looking for more technically oriented answers. I have come across many intellectually challenging problems in my career, but those problems are not the most difficult part of my job. Maybe it is because I enjoy the intellectual problems, but cannot always find ways to solve the people problems.","title":"Data Science Challenges"},{"location":"posts/2017-05-04-death-of-sas/","text":"Originally posted as an answer to Will SAS die in the near future? If history is any indicator, SAS is not going anywhere in the near future. My belief is that open source analytics tools will outpace SAS for new users and use cases, but is less like to displace existing users. I base my opinion by looking at technology trends over the last few decades. As an example, I worked as a data scientist at an insurance company for a few years. The heavily relied on SAS for actuarial and marketing work. While SAS is more costly than open source alternatives, migrating existing work over to new tools would involve a considerable amount of time, money, and effort. This is the same there are still billions of lines of COBOL running in banks and insurance companies across the world. There may be better alternatives, but there is little value in changing something that works. Other examples suggest SAS may need to change its strategy to remain competitive in new markets. Microsoft, once the poster child for proprietary software, is beginning to embrace open source software. I'm not an SAS expert, but the last I checked, they were working on incorporating R and other open source tools into their products.","title":"Will SAS die in the near future?"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/","text":"Originally posted as an answer to How do I migrate MySQL to Hadoop? There are many different ways to migrate MySQL data into Hadoop. Before I can figure out the best way to do that, I need to have a better understanding of your use case. In my experience, there are a few typical use cases. TLDR; \u00b6 Sqoop is your best option if you want to do period data dumps of a MySQL database into a Hadoop cluster. Kafka is your best option if you want to replicate a MySQL database in near real-time, but it adds a lot of additional complexity. Lastly, you can use HBase or Accumulo as a replacement for MySQL, but don't expect the migration to be a simple and easy process. Don't choose this option unless you absolutely need it. Use Case 1: Using Hadoop as a Data Lake with Batch Updates \u00b6 In this use case, you will continue to use the MySQL database, but replicate the data into a Hadoop-based data lake. Simply put, a data lake collects data from a bunch of different data sources and puts them all in the same place. A typical data lake will contain data from multiple different sources. This may include data dumps from relational databases with different vendors. It may include semi-structured data such as web server logs. For this use case, I suggest updating the data in the data lake using a periodic batch process. The batch process can run daily, weekly, or even monthly depending on how often you need to update the data. You can also choose to do incremental updates or full updates. There are various options available to import MySQL data into a Hadoop cluster. Here are a few options I have used to transfer data from relational databases (Oracle and Postgres) to Hadoop. Sqoop is designed as a way to transfer data between Hadoop and structured datastores (e.g. relational databases). In my experience, Sqoop is the best tool to use if you want to reliably replicate an entire database to Hadoop. The Sqoop user guide has information on options for importing data into Hadoop. More than likely, you will want to import the data into Hive or directly into HDFS using a file format like Parquet or Avro. If you are comfortable with using Python, Java, or Scala, you can use Spark to read data from JDBC databases, including MySQL[3]. I would use this option if you would rather keep the data in a MySQL database and just want to do your processing on Hadoop. You can use it to replicate data, but Sqoop is probably a better choice for replication. Embulk is another open-source bulk data transfer tool. Embulk is a good option if you work with a lot of non-Hadoop, non-database sources. As an example, you could use Embulk to store the history of a Slack channel in a database or on a Hadoop cluster. It has plugins for Amazon S3, MySQL, Postgres, Oracle, Google Cloud Storage, Amazon RedShift, MongoDB, Jira, Google Analytics, Elasticsearch, Slack, Google Spreadsheets, and many, many more. I have run into a few issues when trying to import binary data types from Oracle, but I do not know if the MySQL plugin has similar issues. Lastly, you will probably need a scheduler to run the batch jobs. Oozie, Falcon, Azkaban are the three most popular Hadoop workflow engines. If the workflow isn't too complicated, you could also use Cron. Use Case 2: Using Hadoop as a Data Lake with Real-Time Updates \u00b6 If you want to update the data in near real-time, you will probably want to use something like Kafka[4]. A word of warning, updating data in real-time is more complicated than batch updates. I would suggest avoiding real-time updates unless you absolutely need them as they add a lot of extra complexity. Use Case 3: Using Hadoop as a Scalable, Real-time Read/Write Data Store \u00b6 Replacing MySQL (or any OLTP database for that matter) with Hadoop is much more complicated than just using Hadoop as a data lake. Migrating from one database to another (e.g. MySQL to Oracle or MySQL to Postgres) is rarely a trivial task. Migrating a traditional OLTP database to Hadoop is even more difficult. I would not recommend migrating to Hadoop unless you are running into significant issues with your existing solution and cannot find a way to solve them within MySQL. Apache HBase and Apache Accumulo are the two open source, NoSQL data stores that are bundled with most Hadoop distributions. Accumulo and HBase are key/value stores and work differently than your typical relational database. Apache Phoenix adds a user-friendly SQL layer on top of HBase but does not abstract away all of the differences between a key/value store and an RDBMS. In other words, do not expect HBase or Accumulo to function as a drop-in replacement for MySQL. Luckily, if you are replacing MySQL with HBase or Accumulo, you shouldn't need to worry about setting up something to sync the data on a periodic basis. You just need to figure out a way to do a one-time migration of the data. As was the case with batch updates, there a few different ways to migrate the data into HBase or Accumulo. Sqoop supports importing data into both HBase[5] and Accumulo[6]. Since they use a different model from a traditional relational database, you may need to make changes to your existing data model. Pig also supports storing data into HBase[7] and Accumulo[8], but it does not support reading from a MySQL database, so you would have to use Sqoop to transfer the data to Hadoop and then use Pig to store it in HBase or Accumulo. You may want to consider this option if you need to transform your data before storing it in HBase or Accumulo. Summary and Recommendations \u00b6 A Hadoop-based data lake is a great way to get started with Hadoop. Usually, there will be minimal impact on existing operational systems as you can schedule batch jobs during normal system maintenance or during non-peak hours. Sqoop is probably the best tool to use for copying database tables to Hadoop. Updating the data lake in near real-time is possible, but adds a lot of complexity. I recommend using smaller batch times (e.g. hourly instead of daily) instead of more complicated solutions. If you absolutely need faster updates (more than likely you really don't), use something like Kafka. Lastly, replacing a traditional OLTP system like MySQL with Hadoop is not a trivial task. Hadoop's NoSQL datastores work differently than traditional relational databases. If you really do need a Hadoop-based OLTP replacement (you probably don't), then make sure you are willing to commit the required time, expertise, and cost needed to migrate from MySQL to a NoSQL solution.","title":"Migrating MySQL to Hadoop"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/#tldr","text":"Sqoop is your best option if you want to do period data dumps of a MySQL database into a Hadoop cluster. Kafka is your best option if you want to replicate a MySQL database in near real-time, but it adds a lot of additional complexity. Lastly, you can use HBase or Accumulo as a replacement for MySQL, but don't expect the migration to be a simple and easy process. Don't choose this option unless you absolutely need it.","title":"TLDR;"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/#use-case-1-using-hadoop-as-a-data-lake-with-batch-updates","text":"In this use case, you will continue to use the MySQL database, but replicate the data into a Hadoop-based data lake. Simply put, a data lake collects data from a bunch of different data sources and puts them all in the same place. A typical data lake will contain data from multiple different sources. This may include data dumps from relational databases with different vendors. It may include semi-structured data such as web server logs. For this use case, I suggest updating the data in the data lake using a periodic batch process. The batch process can run daily, weekly, or even monthly depending on how often you need to update the data. You can also choose to do incremental updates or full updates. There are various options available to import MySQL data into a Hadoop cluster. Here are a few options I have used to transfer data from relational databases (Oracle and Postgres) to Hadoop. Sqoop is designed as a way to transfer data between Hadoop and structured datastores (e.g. relational databases). In my experience, Sqoop is the best tool to use if you want to reliably replicate an entire database to Hadoop. The Sqoop user guide has information on options for importing data into Hadoop. More than likely, you will want to import the data into Hive or directly into HDFS using a file format like Parquet or Avro. If you are comfortable with using Python, Java, or Scala, you can use Spark to read data from JDBC databases, including MySQL[3]. I would use this option if you would rather keep the data in a MySQL database and just want to do your processing on Hadoop. You can use it to replicate data, but Sqoop is probably a better choice for replication. Embulk is another open-source bulk data transfer tool. Embulk is a good option if you work with a lot of non-Hadoop, non-database sources. As an example, you could use Embulk to store the history of a Slack channel in a database or on a Hadoop cluster. It has plugins for Amazon S3, MySQL, Postgres, Oracle, Google Cloud Storage, Amazon RedShift, MongoDB, Jira, Google Analytics, Elasticsearch, Slack, Google Spreadsheets, and many, many more. I have run into a few issues when trying to import binary data types from Oracle, but I do not know if the MySQL plugin has similar issues. Lastly, you will probably need a scheduler to run the batch jobs. Oozie, Falcon, Azkaban are the three most popular Hadoop workflow engines. If the workflow isn't too complicated, you could also use Cron.","title":"Use Case 1: Using Hadoop as a Data Lake with Batch Updates"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/#use-case-2-using-hadoop-as-a-data-lake-with-real-time-updates","text":"If you want to update the data in near real-time, you will probably want to use something like Kafka[4]. A word of warning, updating data in real-time is more complicated than batch updates. I would suggest avoiding real-time updates unless you absolutely need them as they add a lot of extra complexity.","title":"Use Case 2: Using Hadoop as a Data Lake with Real-Time Updates"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/#use-case-3-using-hadoop-as-a-scalable-real-time-readwrite-data-store","text":"Replacing MySQL (or any OLTP database for that matter) with Hadoop is much more complicated than just using Hadoop as a data lake. Migrating from one database to another (e.g. MySQL to Oracle or MySQL to Postgres) is rarely a trivial task. Migrating a traditional OLTP database to Hadoop is even more difficult. I would not recommend migrating to Hadoop unless you are running into significant issues with your existing solution and cannot find a way to solve them within MySQL. Apache HBase and Apache Accumulo are the two open source, NoSQL data stores that are bundled with most Hadoop distributions. Accumulo and HBase are key/value stores and work differently than your typical relational database. Apache Phoenix adds a user-friendly SQL layer on top of HBase but does not abstract away all of the differences between a key/value store and an RDBMS. In other words, do not expect HBase or Accumulo to function as a drop-in replacement for MySQL. Luckily, if you are replacing MySQL with HBase or Accumulo, you shouldn't need to worry about setting up something to sync the data on a periodic basis. You just need to figure out a way to do a one-time migration of the data. As was the case with batch updates, there a few different ways to migrate the data into HBase or Accumulo. Sqoop supports importing data into both HBase[5] and Accumulo[6]. Since they use a different model from a traditional relational database, you may need to make changes to your existing data model. Pig also supports storing data into HBase[7] and Accumulo[8], but it does not support reading from a MySQL database, so you would have to use Sqoop to transfer the data to Hadoop and then use Pig to store it in HBase or Accumulo. You may want to consider this option if you need to transform your data before storing it in HBase or Accumulo.","title":"Use Case 3: Using Hadoop as a Scalable, Real-time Read/Write Data Store"},{"location":"posts/2017-05-04-migrate-mysql-to-hadoop/#summary-and-recommendations","text":"A Hadoop-based data lake is a great way to get started with Hadoop. Usually, there will be minimal impact on existing operational systems as you can schedule batch jobs during normal system maintenance or during non-peak hours. Sqoop is probably the best tool to use for copying database tables to Hadoop. Updating the data lake in near real-time is possible, but adds a lot of complexity. I recommend using smaller batch times (e.g. hourly instead of daily) instead of more complicated solutions. If you absolutely need faster updates (more than likely you really don't), use something like Kafka. Lastly, replacing a traditional OLTP system like MySQL with Hadoop is not a trivial task. Hadoop's NoSQL datastores work differently than traditional relational databases. If you really do need a Hadoop-based OLTP replacement (you probably don't), then make sure you are willing to commit the required time, expertise, and cost needed to migrate from MySQL to a NoSQL solution.","title":"Summary and Recommendations"},{"location":"posts/2017-05-05-building-analytics-apps/","text":"Originally posted as an answer to What technologies are used to build the sophisticated and advanced client-side dashboards and applications through HTML5 & JavaScript? I can give you a quick overview of some of the technologies I have used to build client-side dashboards and applications. First, I recommend using Scalable Vector Graphics (SVG) for charts, graphs, and other interactive visualizations. SVG works on all modern browsers, supports interactive JavaScript, and scales well to displays of all sizes. While you can create your own code to manipulate SVG visualizations, I do not recommend it. If you are looking to create customized SVG visualizations, you should probably use D3.js - Data-Driven Documents . D3.js is extremely powerful and flexible and is the underlying technology for many plotting and graphing libraries. D3.js is wonderful but does not provide easy to use, out of the box charting and graphing capabilities. I have tried out a few different JavaScript graphing libraries and finally settled on Plot.ly . They provide open source libraries for Python, R, MATLAB, and JavaScript. They also provide a commercial offering for hosted services. Out of all of the solutions I tried, it was the easiest to use out of the box. This covers the charting library but does not provide the overall application platform. There are many options to choose from. Keen IO provides a collection of templates that use Bootstrap. Plot.ly also provides a provides a solution for creating interactive dashboards . If you need something more complicated, you can always use a web application framework like Angular, Polymer, or React. Lastly, there are also options if you are not a strong JavaScript developer. If you are a Python developer, Bokeh uses D3.js and provides a way to create interactive visualization. Similarly, Shiny is an option if you are an R developer.","title":"Building Analytics Apps in HTML5 and JavaScript"}]}