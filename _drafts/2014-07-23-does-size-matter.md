---
layout: post
title: "Does Size Really Matter?"

tags: [metadata]
---
We've probably all heard this discussion before.  Does size really matter?  How big does is it
need to be to be considered Big? Is a Terabyte of data big or do I need to be in the Petabyte range to 
classify it as a "Big Data Problem". Well, I have good news for those you who are insecure about 
the size of your data.  Big Data isn't 
about the size of your data, but how you use it. 

Consider two fictional organizations.  Company A has years of data and lots of it.  Company B is a startup ...
Company A decides to throw all their data into Teradata.  They take a portion of their unstructured data and
make it available via a search engine like Solr or Google Search Appliance. 

Company B uses their data to ...

Company A has higher volume, velocity and variety of data than Company B, but Company B is making more 
effective use of its data.  

Focus on analytics and business intelligence.  These are very good things!

Big Data is about extracting every piece of utility out of your data. 

When people talk about data science, they are usually talking about two different types of people. 
One is more of a traditional statistician.  The other is an engineer. 

Data science is part of a larger trend of software engineering best practices 
crossing over into other disciplines. 

Its really just a trend towards automation. 

Devops